{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6rJIJFU4eL3T"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import glob\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.utils.data as td\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# import torchtext\n",
    "# from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J4hL-cXRePUS"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "vHr9jBEceSQ2",
    "outputId": "1c7bc6e2-fd4a-4c20-bc03-f2923d1dcf0a"
   },
   "outputs": [],
   "source": [
    "TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"),\n",
    "                            #tokenize=get_tokenizer(\"spacy\"),\n",
    "                            init_token='<sos>',\n",
    "                            eos_token='<eos>',\n",
    "                            lower=True)\n",
    "train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)\n",
    "TEXT.build_vocab(train_txt)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    data = TEXT.numericalize([data.examples[0].text])\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_txt, batch_size)\n",
    "val_data = batchify(val_txt, eval_batch_size)\n",
    "test_data = batchify(test_txt, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)\n",
    "modules=list(resnet18.children())[:-1]\n",
    "resnet18=nn.Sequential(*modules).double()\n",
    "for p in resnet18.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(td.Dataset):\n",
    "    def __init__(self, root_dir, mode='train', audio_mode='audio_clean', vid_out='resnet'):\n",
    "        super(MyDataset, self).__init__()\n",
    "        self.mode = mode\n",
    "        self.audio_mode = audio_mode\n",
    "        self.vid_out = vid_out\n",
    "        \n",
    "        if(mode=='train'):\n",
    "            self.folder_dir = os.path.join(root_dir, 'train_npy')\n",
    "            self.maxlen_label=129 \n",
    "            self.maxlen_audio=204 \n",
    "            self.maxlen_video=155 \n",
    "        else:\n",
    "            self.folder_dir = os.path.join(root_dir, 'test_npy')\n",
    "            self.maxlen_label=129 \n",
    "            self.maxlen_audio=204 \n",
    "            self.maxlen_video=155\n",
    "        \n",
    "        self.files = glob.glob(self.folder_dir + \"/*/*.npy\", recursive=True)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"MyDataset(mode={})\".format(self.mode)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        sample = np.load(self.files[idx],allow_pickle=True)\n",
    "\n",
    "        audio_dim = sample.item().get(\"audio_dim\")\n",
    "        audio = sample.item().get(self.audio_mode)\n",
    "        \n",
    "        if(self.maxlen_audio-audio_dim[0]>0):\n",
    "            audio_padding = np.zeros((self.maxlen_audio-audio_dim[0],audio_dim[1]))\n",
    "            audio = np.concatenate((audio,audio_padding),axis=0)\n",
    "\n",
    "        labels_length = sample.item().get(\"labels_length\")\n",
    "        labels = sample.item().get(\"labels\")\n",
    "        \n",
    "        if(self.maxlen_label-labels_length):\n",
    "            label_padding = -np.ones((self.maxlen_label-labels_length))\n",
    "            labels = np.concatenate((labels,label_padding),axis=0)\n",
    "\n",
    "        video_dim = sample.item().get(\"video_dim\")\n",
    "        video = sample.item().get(\"video\")\n",
    "        aus = sample.item().get(\"aus\")\n",
    "        \n",
    "        mean = np.mean(video,axis=(0,1,2))\n",
    "        std = np.std(video,axis=(0,1,2))\n",
    "\n",
    "        new_mean=[0.485, 0.456, 0.406]\n",
    "        new_std=[0.229, 0.224, 0.225]\n",
    "\n",
    "        normalized_video = new_mean + (video-mean)*(new_std/std)\n",
    "\n",
    "        rolled_video = np.rollaxis(normalized_video, 3, 1) \n",
    "        video_dim = (video_dim[0],video_dim[3],video_dim[1],video_dim[2])\n",
    "        \n",
    "        if (self.vid_out=='resnet'):\n",
    "            tensor_video = torch.from_numpy(rolled_video).type(torch.DoubleTensor)\n",
    "            renset_video = resnet18(tensor_video.double()).numpy()\n",
    "            renset_video = renset_video.squeeze((2,3))\n",
    "        \n",
    "            resnet_video_dim = (video_dim[0],512)\n",
    "            \n",
    "            if(self.maxlen_video-video_dim[0]):\n",
    "                video_padding = np.zeros((self.maxlen_video-resnet_video_dim[0],resnet_video_dim[1]))\n",
    "                renset_video = np.concatenate((renset_video,video_padding),axis=0)\n",
    "            return (audio, audio_dim), (renset_video, aus, resnet_video_dim), (labels, labels_length)\n",
    "        else:\n",
    "            if(self.maxlen_video-video_dim[0]):\n",
    "                video_padding = np.zeros((self.maxlen_video-video_dim[0],video_dim[1],video_dim[2],video_dim[3]))\n",
    "                final_video = np.concatenate((rolled_video,video_padding),axis=0)\n",
    "\n",
    "            return (audio, audio_dim), (final_video, aus, video_dim), (labels, labels_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zF_0qLq0eWYZ"
   },
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = MyDataset('/home/pi242/xai/CSE291E_AVSR_project/gdrive',mode='test',vid_out='resnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummydataloader = td.DataLoader(test_data, batch_size=5, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import encoders\n",
    "import decoder\n",
    "importlib.reload(encoders)\n",
    "importlib.reload(decoder)\n",
    "aenc = encoders.AudioTEncoder(240, 512, 4, 512, 4)\n",
    "venc = encoders.VideoTEncoder(512, 512, 4, 512, 4)\n",
    "\n",
    "declayer = decoder.CrossModalTDecoderLayer(512, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0\n[tensor([100,  53,  77, 129,  59]), tensor([240, 240, 240, 240, 240])]\n[tensor([77, 42, 60, 99, 46]), tensor([512, 512, 512, 512, 512])]\ntorch.Size([204, 5, 240]) torch.Size([155, 5, 512])\ntorch.Size([204, 5, 512]) torch.Size([155, 5, 512])\n100\n\ntorch.Size([563, 5, 512])\n"
    }
   ],
   "source": [
    "for i, sample in enumerate(dummydataloader):\n",
    "    print(i)\n",
    "    # print(sample[1][0].shape)\n",
    "    # print(sample[1][2])\n",
    "    # print(sample[0])\n",
    "    print(sample[0][1])\n",
    "    print(sample[1][2])\n",
    "    aip = sample[0][0].permute(1, 0, 2)\n",
    "    vip = sample[1][0].permute(1, 0, 2)\n",
    "    print(aip.shape, vip.shape)\n",
    "\n",
    "    aop = aenc.forward(torch.tensor(aip, dtype=torch.float32), sample[0][1][0])\n",
    "    vop = venc(torch.tensor(vip, dtype=torch.float32), sample[1][2][0])\n",
    "    print(aop.shape, vop.shape)\n",
    "    print(torch.sum(aop[:sample[0][1][0][0].item(), 0, :]))\n",
    "    print()\n",
    "    dummy = declayer.forward(aop, vop, sample[0][1][0], sample[1][2][0])\n",
    "    print(dummy.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "(audio, audio_dim), (final_video, aus, video_dim), (labels, labels_length) = test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(204, 240)\n"
    }
   ],
   "source": [
    "print(audio.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(92, 240) (71, 512) 53\n[17. 16.  7.  1. 22. 10. 17. 23. 21.  3. 16.  6.  1.  7. 11.  9. 10. 22.\n  1. 10. 23. 16.  6. 20.  7.  6.  1.  3. 16.  6.  1. 22. 10. 11. 20. 22.\n 27.  1. 21. 11. 26.  1. 18.  7. 17. 18. 14.  7.  1.  6. 11.  7.  6. -1.\n -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n -1. -1. -1.] 129\n"
    }
   ],
   "source": [
    "print(audio_dim, video_dim, labels_length)\n",
    "print(labels, len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[]\n"
    }
   ],
   "source": [
    "print(aus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    \"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n",
    "        Unmasked positions are filled with float(0.0).\n",
    "    \"\"\"\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[0., -inf, -inf],\n        [0., 0., -inf],\n        [0., 0., 0.]])\n"
    }
   ],
   "source": [
    "print(generate_square_subsequent_mask(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 0.])\n"
    }
   ],
   "source": [
    "a = torch.zeros((10, ))\n",
    "a[2:5] = 1\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CSE291E.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bitf8188a58e4ff44868e3d2910a3581356"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}